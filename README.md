
<p align="center">
  <img src="frontend/assets/banner.png" alt="AI Chatbot Banner" width="80%">
</p>


<h1 align="center">ğŸ¤– AI Chatbot Using LLMs â€” Multi-Domain Intelligent Assistant</h1>

<p align="center">
  <b>FastAPI Backend + Streamlit Frontend + RAG + Groq LLMs</b>
</p>

---

## ğŸ”— **Visit Deployed App**
ğŸ‘‰ **Live Demo:** *<your deployed link here>*

---

## ğŸ“Œ **About the Project**

This project is a **full-stack AI Assistant** built using:

- **FastAPI backend**
- **Groq LLMs** (Llama-3 series)
- **RAG (Retrieval-Augmented Generation)**
- **FAISS vector search**
- **Streamlit frontend** with a stunning cyber-themed UI
- **Domain routing system (Education / Coding / Medical / Legal / General)**

Unlike a simple chatbot, this system is engineered for **multi-domain intelligence**, **contextual reasoning**, **document-aware conversation**, and **production-ready deployment**.

It is designed as a complete template for creating **personal AI assistants**, **enterprise chatbots**, or **educational tools** that blend LLM reasoning with retrieved knowledge.

---

## ğŸš€ **Key Features**

### ğŸŒ Multi-Domain AI Assistant  
Automatically routes queries to specialized agents:
- Education agent  
- Coding/debugging agent  
- Medical Q&A agent  
- Legal understanding agent  
- General reasoning agent  

### ğŸ“š RAG Pipeline (Document-Augmented AI)
Upload PDFs or text files â†’ they are embedded â†’ stored in FAISS â†’ retrieved dynamically.

### âš¡ FastAPI Backend
- REST `/chat` endpoint  
- Streaming WebSocket `/stream` endpoint  
- `/upload` endpoint for RAG documents  
- Metadata routing + confidence scoring  

### ğŸ–¥ï¸ Modern Streamlit Frontend
- Cyberpunk AI theme  
- Animated chat interface  
- Message timestamps  
- Smooth scroll  
- Chat history export (.json / .txt)  
- Sliding sidebar  
- Domain-aware UI highlighting  

### ğŸ”’ Production Ready
- Environment variables  
- Modular architecture  
- Logging  
- Error fallbacks  
- Model deprecation protection  

---

## ğŸ§  **Models Used**

### ğŸ”¹ **Groq LLM (Primary Model)**  
The recommended model:
```

llama3-groq-8b-tool-use-preview

```

Reason:
- Extremely fast inference  
- Strong reasoning  
- Production stable  
- Supported by Groqâ€™s latest API  

### ğŸ”¹ Sentence Transformer (Embeddings for RAG)
```

sentence-transformers/all-MiniLM-L6-v2

```

Used for:
- Document chunk embedding  
- Semantic similarity search  

### ğŸ”¹ FAISS (Vector Store)
Used for fast retrieval of relevant knowledge chunks.

---

## ğŸ—ï¸ **System Architecture**

```

```
             USER
               â”‚
               â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Streamlit UI  â”‚
      â”‚ (Chat + Upload)â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ REST / WS
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        FastAPI        â”‚
    â”‚  /chat /stream /uploadâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â–¼             â–¼             â–¼
```

Domain Router   Agents        RAG Engine
(LLM-based)   (5 domains)    (FAISS + Embeddings)
â”‚
â–¼
Groq LLM API

```

---

## ğŸ“‚ **Project Structure**

```

AI_Chatbot_using_LLMs/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ server.py
â”‚   â”‚   â”œâ”€â”€ upload.py
â”‚   â”‚   â”œâ”€â”€ deps.py
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”‚   â”œâ”€â”€ education_agent.py
â”‚   â”‚   â”œâ”€â”€ coding_agent.py
â”‚   â”‚   â”œâ”€â”€ medical_agent.py
â”‚   â”‚   â”œâ”€â”€ legal_agent.py
â”‚   â”‚   â”œâ”€â”€ general_agent.py
â”‚   â”‚   â””â”€â”€ prompts/
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py
â”‚   â”‚   â”œâ”€â”€ loader.py
â”‚   â”‚   â””â”€â”€ embedder.py
â”‚   â”‚
â”‚   â”œâ”€â”€ router/
â”‚   â”‚   â””â”€â”€ domain_router.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ llm.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ context_manager.py
â”‚   â”‚
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ static/style.css
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

# âš™ï¸ **Setup Instructions (Local Development)**

## 1ï¸âƒ£ **Clone the Repository**
```bash
git clone <your repo link>
cd AI_Chatbot_using_LLMs
````

---

# ğŸ” 2ï¸âƒ£ Create Environment Variables (`.env`)

Create a `.env` file:

```
GROQ_API_KEY=your_key_here
GROQ_MODEL=llama3-groq-8b-tool-use-preview
```

---

# ğŸ 3ï¸âƒ£ Backend Setup (FastAPI)

### Create Conda environment:

```bash
conda create -n llm_backend python=3.10
conda activate llm_backend
```

### Install dependencies:

```bash
pip install -r requirements.txt
```

### Start backend:

```bash
uvicorn src.api.server:app --reload
```

Backend runs at:

```
http://127.0.0.1:8000
```

Test endpoint:

```bash
curl -X POST http://127.0.0.1:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"session_id":"test","message":"hello"}'
```

---

# ğŸ–¥ï¸ 4ï¸âƒ£ Frontend Setup (Streamlit)

Navigate to frontend:

```bash
cd frontend
streamlit run app.py
```

Frontend runs at:

```
http://localhost:8501
```

---

## ğŸ§ª **API Endpoints**

### POST `/chat`

```
{
  "session_id": "123",
  "message": "Explain OOP"
}
```

### POST `/upload`

Upload PDFs or text for RAG.

### WebSocket `/stream`

For token streaming responses.

---

## ğŸš€ Deployment Guide

### Deploy Backend:

* Render
* Railway
* Azure App Service
* AWS EC2
* Docker container

### Deploy Frontend:

* Streamlit Community Cloud
* Docker container
* CloudRun

---

## ğŸ¤ Contributing

Pull requests are welcome.
For major changes, open an issue first to discuss the proposal.

---

## ğŸ“„ License

MIT License (or your choice)

---

## â­ Support This Project

If this project helped you, consider giving a **star** â­ on GitHub!
